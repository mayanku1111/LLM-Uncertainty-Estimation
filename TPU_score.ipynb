{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb36f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pip install transformers accelerate scikit-learn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58898374",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e12b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "    \"question\": \"In the context of time complexity, which of the following sorting algorithms has a worst-case performance of O(n log n)?\",\n",
    "    \"choices\": [\n",
    "        \"QuickSort\",\n",
    "        \"MergeSort\",\n",
    "        \"BubbleSort\",\n",
    "        \"InsertionSort\"\n",
    "    ],\n",
    "    \"answer_idx\": 1,\n",
    "    },\n",
    "    {\n",
    "    \"question\": \"The Treaty of Versailles, which formally ended World War I, was signed in which year?\",\n",
    "    \"choices\": [\"1917\", \"1918\", \"1919\", \"1920\"],\n",
    "    \"answer_idx\": 2,\n",
    "    },\n",
    "    {\n",
    "    \"question\": \"In the U.S. legal system, the 'exclusionary rule' primarily prohibits the use of what in a criminal trial?\",\n",
    "    \"choices\": [\n",
    "        \"Hearsay evidence\",\n",
    "        \"Evidence obtained through an unlawful search and seizure\",\n",
    "        \"Testimony from an accomplice\",\n",
    "        \"Circumstantial evidence\"\n",
    "    ],\n",
    "    \"answer_idx\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which of the following is a prime number?\",\n",
    "        \"choices\": [\"21\", \"27\", \"29\", \"33\"],\n",
    "        \"answer_idx\": 2\n",
    "    },\n",
    "]\n",
    "\n",
    "letters = \"ABCD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6b643",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_prompt(q, choices):\n",
    "    choices_str = \"\\n\".join(f\"{letters[i]}) {choices[i]}\" for i in range(len(choices)))\n",
    "    return f\"Question: {q}\\n\\nChoices:\\n{choices_str}\\n\\nAnswer with a single letter (A, B, C, ...).\\nAnswer:\"\n",
    "\n",
    "prompts = [build_prompt(e[\"question\"], e[\"choices\"]) for e in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a86eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=32,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    temperature=0.0  # greedy\n",
    ")\n",
    "\n",
    "# transition_scores: (batch, gen_len)\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8573f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# exclude prompt tokens\n",
    "input_lens = (inputs[\"input_ids\"] != tokenizer.pad_token_id).sum(dim=1).tolist()\n",
    "gen_logprobs_batch = []\n",
    "decoded_batch = []\n",
    "for b in range(len(prompts)):\n",
    "    start = input_lens[b]\n",
    "    gen_logprobs = transition_scores[b, start:]\n",
    "    gen_logprobs_batch.append(gen_logprobs.cpu().numpy())\n",
    "    decoded = tokenizer.decode(outputs.sequences[b, start:], skip_special_tokens=True)\n",
    "    decoded_batch.append(decoded.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0079d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TPU per example\n",
    "def compute_tpu(logprobs):\n",
    "    if len(logprobs) == 0:\n",
    "        return float(\"nan\")\n",
    "    avg_log = np.mean(logprobs)\n",
    "    return 1.0 - math.exp(avg_log)\n",
    "\n",
    "tpu_values = [compute_tpu(lp) for lp in gen_logprobs_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d84ba4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parse model answers\n",
    "def parse_answer(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    first = text.strip()[0].upper()\n",
    "    if first in letters:\n",
    "        return letters.index(first)\n",
    "    return None\n",
    "\n",
    "pred_indices = [parse_answer(out) for out in decoded_batch]\n",
    "correct_mask = [\n",
    "    (pred is not None and pred == ex[\"answer_idx\"])\n",
    "    for pred, ex in zip(pred_indices, examples)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d5dcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics: ECE & AUROC\n",
    "def expected_calibration_error(unc, correct, n_bins=10):\n",
    "    unc = np.asarray(unc)\n",
    "    correct = np.asarray(correct)\n",
    "    valid = ~np.isnan(unc)\n",
    "    unc, correct = unc[valid], correct[valid]\n",
    "    if len(unc) == 0: return float(\"nan\")\n",
    "\n",
    "    bins = np.linspace(0, 1, n_bins+1)\n",
    "    bin_idx = np.digitize(unc, bins) - 1\n",
    "    ece = 0.0\n",
    "    n = len(unc)\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_idx == b\n",
    "        if mask.sum() == 0: continue\n",
    "        acc = correct[mask].mean()\n",
    "        avg_unc = unc[mask].mean()\n",
    "        # predicted confidence = 1 - unc\n",
    "        ece += (mask.sum()/n) * abs(acc - (1.0 - avg_unc))\n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead9402",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ece = expected_calibration_error(tpu_values, correct_mask, n_bins=10)\n",
    "y_true = (~np.array(correct_mask)).astype(int)  # incorrect=1\n",
    "try:\n",
    "    auroc = roc_auc_score(y_true, np.nan_to_num(tpu_values))\n",
    "except Exception:\n",
    "    auroc = float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a31314",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"\\nResults per example:\")\n",
    "for i, ex in enumerate(examples):\n",
    "    print(f\"Q: {ex['question']}\")\n",
    "    print(f\"Model output: {decoded_batch[i]}\")\n",
    "    print(f\"Pred idx: {pred_indices[i]}, GT idx: {ex['answer_idx']}, Correct: {correct_mask[i]}\")\n",
    "    print(f\"TPU: {tpu_values[i]:.4f}\\n\")\n",
    "\n",
    "print(\"Metrics:\")\n",
    "print(f\"ECE:   {ece:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f75eb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reliability_diagram(unc, correct, n_bins=10):\n",
    "    unc = np.asarray(unc)\n",
    "    correct = np.asarray(correct)\n",
    "    valid = ~np.isnan(unc)\n",
    "    unc, correct = unc[valid], correct[valid]\n",
    "    if len(unc) == 0:\n",
    "        print(\"No valid data to plot\")\n",
    "        return\n",
    "    \n",
    "    conf = 1.0 - unc\n",
    "    \n",
    "    # Bin by confidence\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    bin_idx = np.digitize(conf, bins) - 1\n",
    "    bin_acc = []\n",
    "    bin_conf = []\n",
    "    \n",
    "    for b in range(n_bins):\n",
    "        mask = bin_idx == b\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        acc = correct[mask].mean()\n",
    "        avg_conf = conf[mask].mean()\n",
    "        bin_acc.append(acc)\n",
    "        bin_conf.append(avg_conf)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot([0,1],[0,1], 'k--', label=\"Perfect calibration\")\n",
    "    plt.plot(bin_conf, bin_acc, marker='o', label=\"TPU\")\n",
    "    plt.xlabel(\"Predicted confidence (1 - TPU)\")\n",
    "    plt.ylabel(\"Empirical accuracy\")\n",
    "    plt.title(\"Reliability Diagram (TPU)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_reliability_diagram(tpu_values, correct_mask, n_bins=10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
