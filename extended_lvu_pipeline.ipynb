{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df540fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch transformers accelerate sentencepiece bitsandbytes scikit-learn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30758f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelCfg:\n",
    "    model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    device_map: str = \"auto\"\n",
    "    torch_dtype: str = \"bfloat16\"\n",
    "    load_in_4bit: bool = False\n",
    "    max_new_tokens: int = 256\n",
    "    temperature: float = 0.0\n",
    "    top_p: float = 1.0\n",
    "\n",
    "class Llama31Judge:\n",
    "    def __init__(self, cfg: ModelCfg = ModelCfg()):\n",
    "        quant_args = {}\n",
    "        if cfg.load_in_4bit:\n",
    "            quant_args = dict(load_in_4bit=True, bnb_4bit_compute_dtype=getattr(torch, cfg.torch_dtype))\n",
    "        self.tok = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            cfg.model_name,\n",
    "            device_map=cfg.device_map,\n",
    "            torch_dtype=getattr(torch, cfg.torch_dtype),\n",
    "            **quant_args\n",
    "        )\n",
    "        self.gen_cfg = GenerationConfig(\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            temperature=cfg.temperature,\n",
    "            top_p=cfg.top_p,\n",
    "            do_sample=cfg.temperature > 0\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, max_new_tokens: Optional[int] = None) -> str:\n",
    "        cfg = self.gen_cfg\n",
    "        if max_new_tokens:\n",
    "            cfg = GenerationConfig(**{**self.gen_cfg.to_dict(), \"max_new_tokens\": max_new_tokens})\n",
    "        inputs = self.tok(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        out = self.model.generate(**inputs, generation_config=cfg)\n",
    "        text = self.tok.decode(out[0], skip_special_tokens=True)\n",
    "        return text[len(self.tok.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)) :].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SHARED_SAMPLING_PROMPT = \"\"\"Read the question, analyze step by step, provide your answer and your confidence in this\n",
    "answer. Note: The confidence indicates how likely you think your answer is true.\n",
    "If you are uncertain about your answer to the question, convey this uncertainty linguistically\n",
    "by precisely hedging this answer. Use the following format to answer:\n",
    "Explanation: [insert short step-by-step analysis here]\n",
    "Answer: [ONLY the option LETTER, where the LETTER is one of {Letters}]\n",
    "Confidence (0-100): [Your confidence level, please only include the numerical number in the\n",
    "range of 0-100]%\n",
    "\n",
    "Only give me the reply according to this format, donâ€™t give me any other words.\n",
    "\n",
    "{Question}\n",
    "\"\"\"\n",
    "\n",
    "LVU_JUDGE_PROMPT = \"\"\"You will be given a question and a proposed answer generated by a language model. Your\n",
    "task is to output an confidence score: a number in [0,1] representing how certain the language\n",
    "model is about the truthfulness of its generated answer, judged solely based on the provided\n",
    "answer. You should pay special attention to the usage of any hedging modifiers in the original\n",
    "answer, used to convey uncertainty in the truthfulness of the answer. A score close to 1.0\n",
    "indicates that the model is very certain about the truthfulness answer. And a score close to\n",
    "0.0 means that the model is highly uncertain, and the answer likely contains some hedging expressions. If the proposed answer punts the question, the confidence score should be 0.0.\n",
    "\n",
    "Here are the question and proposed answer to be judged:\n",
    "Question:\n",
    "{Question}\n",
    "Proposed Answer:\n",
    "{Response}\n",
    "\n",
    "Provide your response only in the following format:\n",
    "Confidence score: [confidence score (0-1)].\n",
    "\"\"\"\n",
    "\n",
    "HEDGING_PROMPT = \"\"\"{sentence}\n",
    "From their perspective, considering the use of hedging words, what is the probability (from 0\n",
    "to 100) that the statement is true? Concisely provide your response only in the following\n",
    "format:\n",
    "Probability: [0-100]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ANSWER_REGEXES = [\n",
    "    r\"[Aa]nswer:?[\\s]*[\n",
    "]*([A-J])\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*\\(?([A-J])\\)?\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*\\[?([A-J])\\]?\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*([A-J])[,)]\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*([A-J])\\s*,?.*\",\n",
    "    r\"Answer:\\n([A-J])\\nConfidence\",\n",
    "    r\"answer is\\s*\\[?\\(?([A-J])\\]?\\)?\",\n",
    "    r\"answer should be\\s*\\[?\\(?([A-J])\\]?\\)?\",\n",
    "    r\"best option is \\(?([A-J])\\)?\",\n",
    "    r\"best match is option \\(?([A-J])\\)?\",\n",
    "    r\"the closest is \\(?([A-J])\\)?\",\n",
    "    r\"Answer:\\n*^([A-J])$\",\n",
    "    r\"^([A-J])$\",\n",
    "]\n",
    "\n",
    "CONF_REGEXES = [\n",
    "    r\"[Cc]onfidence\\s*\\(0-100\\):\\s*[\\(]?[\\[]?(\\d+)[\\)]?[\\]]?%?\",\n",
    "    r\"[Cc]onfidence[:]?:\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence [\\(0-100\\)]?:\\s*\\[(\\d+)%?\\]\",\n",
    "    r\"[Cc]onfidence [Ll]evel\\s*\\(0-100\\):\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence [Ll]evel[:]?:\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence [Ll]evel[\\(0-100\\)]?:\\s*\\[(\\d+)%?\\]\",\n",
    "    r\"[Cc]onfidence \\(100\\):\\s*\\w*,\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence\\s*\\(\\d+\\)\\s*:\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence\\s*[\\(]?(\\d+)[\\)]?%?\",\n",
    "]\n",
    "\n",
    "HEDGE_REGEX = re.compile(r\"\\b(?:Probability|Prob(?:\\.|ability)?|P)\\s*[:=]?\\s*(\\d+)%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac27b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_answer_letter(text: str) -> Optional[str]:\n",
    "    for rx in ANSWER_REGEXES:\n",
    "        m = re.search(rx, text, flags=re.MULTILINE)\n",
    "        if m:\n",
    "            return m.group(1).strip().upper()\n",
    "    return None\n",
    "\n",
    "def strip_numeric_confidence(text: str) -> Tuple[str, Optional[int]]:\n",
    "    conf_val = None\n",
    "    cleaned = text\n",
    "    for rx in CONF_REGEXES:\n",
    "        m = re.search(rx, cleaned, flags=re.MULTILINE)\n",
    "        if m:\n",
    "            try:\n",
    "                conf_val = int(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "            cleaned = re.sub(rx, \"\", cleaned)\n",
    "    return cleaned.strip(), conf_val\n",
    "\n",
    "def parse_lvu_conf(output: str) -> Optional[float]:\n",
    "    m = re.search(r\"Confidence score:\\s*([01](?:\\.\\d+)?)\", output)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def parse_hedge_conf(output: str) -> Optional[int]:\n",
    "    m = HEDGE_REGEX.search(output)\n",
    "    return int(m.group(1)) if m else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_ece(conf: List[float], correct: List[int], bins: int = 10) -> float:\n",
    "    n = len(conf)\n",
    "    ece = 0.0\n",
    "    for b in range(bins):\n",
    "        lo, hi = b / bins, (b + 1) / bins\n",
    "        idx = [i for i, c in enumerate(conf) if lo <= c <= hi]\n",
    "        if not idx:\n",
    "            continue\n",
    "        acc = sum(correct[i] for i in idx) / len(idx)\n",
    "        avg_conf = sum(conf[i] for i in idx) / len(idx)\n",
    "        ece += (len(idx) / n) * abs(acc - avg_conf)\n",
    "    return ece\n",
    "\n",
    "def compute_auroc(conf: List[float], correct: List[int]) -> Optional[float]:\n",
    "    if len(set(correct)) < 2:\n",
    "        return None\n",
    "    return roc_auc_score(correct, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ccbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = ModelCfg(load_in_4bit=True)\n",
    "mdl = Llama31Judge(cfg)\n",
    "\n",
    "q = \"Which gas is most responsible for the greenhouse effect on Earth?\\nA) Oxygen\\nB) Nitrogen\\nC) Carbon dioxide\\nD) Argon\"\n",
    "letters = \"ABCD\"\n",
    "\n",
    "resp = mdl.generate(SHARED_SAMPLING_PROMPT.format(Letters=letters, Question=q))\n",
    "\n",
    "pred_letter = extract_answer_letter(resp)\n",
    "\n",
    "cleaned, nvu = strip_numeric_confidence(resp)\n",
    "\n",
    "judge_prompt = LVU_JUDGE_PROMPT.format(Question=q, Response=cleaned)\n",
    "judge_out = mdl.generate(judge_prompt, max_new_tokens=64)\n",
    "lvu_conf = parse_lvu_conf(judge_out)\n",
    "\n",
    "print(\"RAW RESPONSE:\\n\", resp)\n",
    "print(\"Predicted Answer Letter:\", pred_letter)\n",
    "print(\"NVU (numeric) if present:\", nvu)\n",
    "print(\"LVU Judge Output:\", judge_out)\n",
    "print(\"LVU Confidence [0-1]:\", lvu_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = [\"Brendan\", \"Amanda\"]\n",
    "hedges = [\"almost certain\", \"possible\", \"unlikely\"]\n",
    "statements = [\"they will buy a new watch this Thanksgiving weekend.\", \"their boss owns a blue car.\"]\n",
    "\n",
    "for n in names:\n",
    "    for h in hedges:\n",
    "        for s in statements:\n",
    "            sentence = f\"{n} believes it is {h} that {s}\"\n",
    "            out = mdl.generate(HEDGING_PROMPT.format(sentence=sentence), max_new_tokens=32)\n",
    "            prob = parse_hedge_conf(out)\n",
    "            print(sentence, \"->\", out, \"| Parsed:\", prob)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
