{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15babf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch transformers accelerate sentencepiece bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelCfg:\n",
    "    model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    device_map: str = \"auto\"\n",
    "    torch_dtype: str = \"bfloat16\"\n",
    "    load_in_4bit: bool = False\n",
    "    max_new_tokens: int = 256\n",
    "    temperature: float = 0.0\n",
    "    top_p: float = 1.0\n",
    "\n",
    "class Llama31Judge:\n",
    "    def __init__(self, cfg: ModelCfg = ModelCfg()):\n",
    "        quant_args = {}\n",
    "        if cfg.load_in_4bit:\n",
    "            quant_args = dict(load_in_4bit=True, bnb_4bit_compute_dtype=getattr(torch, cfg.torch_dtype))\n",
    "        self.tok = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            cfg.model_name,\n",
    "            device_map=cfg.device_map,\n",
    "            torch_dtype=getattr(torch, cfg.torch_dtype),\n",
    "            **quant_args\n",
    "        )\n",
    "        self.gen_cfg = GenerationConfig(\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            temperature=cfg.temperature,\n",
    "            top_p=cfg.top_p,\n",
    "            do_sample=cfg.temperature > 0\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        inputs = self.tok(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        out = self.model.generate(**inputs, generation_config=self.gen_cfg)\n",
    "        text = self.tok.decode(out[0], skip_special_tokens=True)\n",
    "        return text[len(self.tok.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)) :].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac893f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SHARED_SAMPLING_PROMPT = \"\"\"Read the question, analyze step by step, provide your answer and your confidence in this\n",
    "answer. Note: The confidence indicates how likely you think your answer is true.\n",
    "If you are uncertain about your answer to the question, convey this uncertainty linguistically\n",
    "by precisely hedging this answer. Use the following format to answer:\n",
    "Explanation: [insert short step-by-step analysis here]\n",
    "Answer: [ONLY the option LETTER, where the LETTER is one of {Letters}]\n",
    "Confidence (0-100): [Your confidence level, please only include the numerical number in the\n",
    "range of 0-100]%\n",
    "\n",
    "Only give me the reply according to this format, donâ€™t give me any other words.\n",
    "\n",
    "{Question}\n",
    "\"\"\"\n",
    "\n",
    "LVU_JUDGE_PROMPT = \"\"\"You will be given a question and a proposed answer generated by a language model. Your\n",
    "task is to output an confidence score: a number in [0,1] representing how certain the language\n",
    "model is about the truthfulness of its generated answer, judged solely based on the provided\n",
    "answer. You should pay special attention to the usage of any hedging modifiers in the original\n",
    "answer, used to convey uncertainty in the truthfulness of the answer. A score close to 1.0\n",
    "indicates that the model is very certain about the truthfulness answer. And a score close to\n",
    "0.0 means that the model is highly uncertain, and the answer likely contains some hedging expressions. If the proposed answer punts the question, the confidence score should be 0.0.\n",
    "\n",
    "Here are the question and proposed answer to be judged:\n",
    "Question:\n",
    "{Question}\n",
    "Proposed Answer:\n",
    "{Response}\n",
    "\n",
    "Provide your response only in the following format:\n",
    "Confidence score: [confidence score (0-1)].\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ANSWER_REGEXES = [\n",
    "    r\"[Aa]nswer:?[\\s]*[\n",
    "]*([A-J])\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*\\(?([A-J])\\)?\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*\\[?([A-J])\\]?\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*([A-J])[,)]\",\n",
    "    r\"[Aa]nswer:[\\s]*[\n",
    "]*([A-J])\\s*,?.*\",\n",
    "    r\"Answer:\\n([A-J])\\nConfidence\",\n",
    "    r\"answer is\\s*\\[?\\(?([A-J])\\]?\\)?\",\n",
    "    r\"answer should be\\s*\\[?\\(?([A-J])\\]?\\)?\",\n",
    "    r\"best option is \\(?([A-J])\\)?\",\n",
    "    r\"best match is option \\(?([A-J])\\)?\",\n",
    "    r\"the closest is \\(?([A-J])\\)?\",\n",
    "    r\"Answer:\\n*^([A-J])$\",\n",
    "    r\"^([A-J])$\",\n",
    "]\n",
    "CONF_STRIP_REGEXES = [\n",
    "    r\"[Cc]onfidence\\s*\\(0-100\\):\\s*[\\(]?[\\[]?(\\d+)[\\)]?[\\]]?%?\",\n",
    "    r\"[Cc]onfidence[:]?:\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence [\\(0-100\\)]?:\\s*\\[(\\d+)%?\\]\",\n",
    "    r\"[Cc]onfidence [Ll]evel\\s*\\(0-100\\):\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence [Ll]evel[:]?:\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence [Ll]evel[\\(0-100\\)]?:\\s*\\[(\\d+)%?\\]\",\n",
    "    r\"[Cc]onfidence \\(100\\):\\s*\\w*,\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence\\s*\\(\\d+\\)\\s*:\\s*(\\d+)%?\",\n",
    "    r\"[Cc]onfidence\\s*[\\(]?(\\d+)[\\)]?%?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_letter(response_text: str) -> Optional[str]:\n",
    "    for pat in ANSWER_REGEXES:\n",
    "        m = re.search(pat, response_text, flags=re.MULTILINE)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def strip_numeric_confidence(response_text: str) -> Tuple[str, Optional[int]]:\n",
    "    conf_val = None\n",
    "    cleaned = response_text\n",
    "    for pat in CONF_STRIP_REGEXES:\n",
    "        m = re.search(pat, cleaned, flags=re.MULTILINE)\n",
    "        if m:\n",
    "            try:\n",
    "                conf_val = int(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "            cleaned = re.sub(pat, \"\", cleaned)\n",
    "    return cleaned.strip(), conf_val\n",
    "\n",
    "def parse_judge_confidence(judge_output: str) -> Optional[float]:\n",
    "    m = re.search(r\"Confidence score:\\s*([01](?:\\.\\d+)?)\", judge_output)\n",
    "    return float(m.group(1)) if m else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce0a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = ModelCfg(load_in_8bit=True)  \n",
    "mdl = Llama31Judge(cfg)\n",
    "\n",
    "# Example MMLU-style question\n",
    "q = \"Which gas is most responsible for the greenhouse effect on Earth?\\nA) Oxygen\\nB) Nitrogen\\nC) Carbon dioxide\\nD) Argon\"\n",
    "letters = \"ABCD\"\n",
    "\n",
    "# 1) Sample response\n",
    "resp = mdl.generate(SHARED_SAMPLING_PROMPT.format(Letters=letters, Question=q))\n",
    "\n",
    "# 2) Extract answer letter\n",
    "ans = extract_letter(resp)\n",
    "\n",
    "# 3) Remove numeric confidence before LVU judging\n",
    "cleaned, nvu = strip_numeric_confidence(resp)\n",
    "\n",
    "# 4) Judge LVU confidence\n",
    "judge_prompt = LVU_JUDGE_PROMPT.format(Question=q, Response=cleaned)\n",
    "judge_out = mdl.generate(judge_prompt)\n",
    "lvu = parse_judge_confidence(judge_out)\n",
    "\n",
    "print(\"RAW RESPONSE:\\n\", resp)\n",
    "print(\"Predicted Letter:\", ans)\n",
    "print(\"Numeric Verbal Confidence:\", nvu)\n",
    "print(\"LVU Judge Output:\", judge_out)\n",
    "print(\"LVU Confidence [0-1]:\", lvu)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
