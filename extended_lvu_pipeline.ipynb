{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "5ff3b4c5", "cell_type": "markdown", "source": "\n# Extended LVU Pipeline with Llama-3.1-8B\n\nThis notebook fully implements the **Linguistic Verbal Uncertainty (LVU)** pipeline for MMLU-Pro style evaluation, \nfollowing the methodology and regex sets from the research paper *Revisiting Uncertainty Estimation and Calibration of Large Language Models* (2025).\n\nIt includes:\n- Shared response prompt (with NVU + LVU + concise CoT)\n- Regex-based extraction of answers and numeric confidence\n- LVU judge with hedging sensitivity\n- Example evaluation loop with metrics (ECE, AUROC)\n- Optional hedging-word experiment (Appendix H)\n", "metadata": {}}, {"id": "df540fe4", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n!pip install torch transformers accelerate sentencepiece bitsandbytes scikit-learn tqdm\n", "outputs": []}, {"id": "fa77d10e", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport re\nimport json\nimport math\nfrom typing import Optional, Tuple, Dict, List\nfrom dataclasses import dataclass\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\n", "outputs": []}, {"id": "30758f9e", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n@dataclass\nclass ModelCfg:\n    model_name: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    device_map: str = \"auto\"\n    torch_dtype: str = \"bfloat16\"\n    load_in_4bit: bool = False\n    max_new_tokens: int = 256\n    temperature: float = 0.0\n    top_p: float = 1.0\n\nclass Llama31Judge:\n    def __init__(self, cfg: ModelCfg = ModelCfg()):\n        quant_args = {}\n        if cfg.load_in_4bit:\n            quant_args = dict(load_in_4bit=True, bnb_4bit_compute_dtype=getattr(torch, cfg.torch_dtype))\n        self.tok = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            cfg.model_name,\n            device_map=cfg.device_map,\n            torch_dtype=getattr(torch, cfg.torch_dtype),\n            **quant_args\n        )\n        self.gen_cfg = GenerationConfig(\n            max_new_tokens=cfg.max_new_tokens,\n            temperature=cfg.temperature,\n            top_p=cfg.top_p,\n            do_sample=cfg.temperature > 0\n        )\n\n    @torch.inference_mode()\n    def generate(self, prompt: str, max_new_tokens: Optional[int] = None) -> str:\n        cfg = self.gen_cfg\n        if max_new_tokens:\n            cfg = GenerationConfig(**{**self.gen_cfg.to_dict(), \"max_new_tokens\": max_new_tokens})\n        inputs = self.tok(prompt, return_tensors=\"pt\").to(self.model.device)\n        out = self.model.generate(**inputs, generation_config=cfg)\n        text = self.tok.decode(out[0], skip_special_tokens=True)\n        return text[len(self.tok.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)) :].strip()\n", "outputs": []}, {"id": "040c324d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nSHARED_SAMPLING_PROMPT = \"\"\"Read the question, analyze step by step, provide your answer and your confidence in this\nanswer. Note: The confidence indicates how likely you think your answer is true.\nIf you are uncertain about your answer to the question, convey this uncertainty linguistically\nby precisely hedging this answer. Use the following format to answer:\nExplanation: [insert short step-by-step analysis here]\nAnswer: [ONLY the option LETTER, where the LETTER is one of {Letters}]\nConfidence (0-100): [Your confidence level, please only include the numerical number in the\nrange of 0-100]%\n\nOnly give me the reply according to this format, don\u2019t give me any other words.\n\n{Question}\n\"\"\"\n\nLVU_JUDGE_PROMPT = \"\"\"You will be given a question and a proposed answer generated by a language model. Your\ntask is to output an confidence score: a number in [0,1] representing how certain the language\nmodel is about the truthfulness of its generated answer, judged solely based on the provided\nanswer. You should pay special attention to the usage of any hedging modifiers in the original\nanswer, used to convey uncertainty in the truthfulness of the answer. A score close to 1.0\nindicates that the model is very certain about the truthfulness answer. And a score close to\n0.0 means that the model is highly uncertain, and the answer likely contains some hedging expressions. If the proposed answer punts the question, the confidence score should be 0.0.\n\nHere are the question and proposed answer to be judged:\nQuestion:\n{Question}\nProposed Answer:\n{Response}\n\nProvide your response only in the following format:\nConfidence score: [confidence score (0-1)].\n\"\"\"\n\nHEDGING_PROMPT = \"\"\"{sentence}\nFrom their perspective, considering the use of hedging words, what is the probability (from 0\nto 100) that the statement is true? Concisely provide your response only in the following\nformat:\nProbability: [0-100]\n\"\"\"\n", "outputs": []}, {"id": "bee4d8a5", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nANSWER_REGEXES = [\n    r\"[Aa]nswer:?[\\s]*[\n]*([A-J])\",\n    r\"[Aa]nswer:[\\s]*[\n]*\\(?([A-J])\\)?\",\n    r\"[Aa]nswer:[\\s]*[\n]*\\[?([A-J])\\]?\",\n    r\"[Aa]nswer:[\\s]*[\n]*([A-J])[,)]\",\n    r\"[Aa]nswer:[\\s]*[\n]*([A-J])\\s*,?.*\",\n    r\"Answer:\\n([A-J])\\nConfidence\",\n    r\"answer is\\s*\\[?\\(?([A-J])\\]?\\)?\",\n    r\"answer should be\\s*\\[?\\(?([A-J])\\]?\\)?\",\n    r\"best option is \\(?([A-J])\\)?\",\n    r\"best match is option \\(?([A-J])\\)?\",\n    r\"the closest is \\(?([A-J])\\)?\",\n    r\"Answer:\\n*^([A-J])$\",\n    r\"^([A-J])$\",\n]\n\nCONF_REGEXES = [\n    r\"[Cc]onfidence\\s*\\(0-100\\):\\s*[\\(]?[\\[]?(\\d+)[\\)]?[\\]]?%?\",\n    r\"[Cc]onfidence[:]?:\\s*(\\d+)%?\",\n    r\"[Cc]onfidence [\\(0-100\\)]?:\\s*\\[(\\d+)%?\\]\",\n    r\"[Cc]onfidence [Ll]evel\\s*\\(0-100\\):\\s*(\\d+)%?\",\n    r\"[Cc]onfidence [Ll]evel[:]?:\\s*(\\d+)%?\",\n    r\"[Cc]onfidence [Ll]evel[\\(0-100\\)]?:\\s*\\[(\\d+)%?\\]\",\n    r\"[Cc]onfidence \\(100\\):\\s*\\w*,\\s*(\\d+)%?\",\n    r\"[Cc]onfidence\\s*\\(\\d+\\)\\s*:\\s*(\\d+)%?\",\n    r\"[Cc]onfidence\\s*[\\(]?(\\d+)[\\)]?%?\",\n]\n\nHEDGE_REGEX = re.compile(r\"\\b(?:Probability|Prob(?:\\.|ability)?|P)\\s*[:=]?\\s*(\\d+)%\")\n", "outputs": []}, {"id": "fac27b31", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndef extract_answer_letter(text: str) -> Optional[str]:\n    for rx in ANSWER_REGEXES:\n        m = re.search(rx, text, flags=re.MULTILINE)\n        if m:\n            return m.group(1).strip().upper()\n    return None\n\ndef strip_numeric_confidence(text: str) -> Tuple[str, Optional[int]]:\n    conf_val = None\n    cleaned = text\n    for rx in CONF_REGEXES:\n        m = re.search(rx, cleaned, flags=re.MULTILINE)\n        if m:\n            try:\n                conf_val = int(m.group(1))\n            except Exception:\n                pass\n            cleaned = re.sub(rx, \"\", cleaned)\n    return cleaned.strip(), conf_val\n\ndef parse_lvu_conf(output: str) -> Optional[float]:\n    m = re.search(r\"Confidence score:\\s*([01](?:\\.\\d+)?)\", output)\n    return float(m.group(1)) if m else None\n\ndef parse_hedge_conf(output: str) -> Optional[int]:\n    m = HEDGE_REGEX.search(output)\n    return int(m.group(1)) if m else None\n", "outputs": []}, {"id": "f264e817", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndef compute_ece(conf: List[float], correct: List[int], bins: int = 10) -> float:\n    n = len(conf)\n    ece = 0.0\n    for b in range(bins):\n        lo, hi = b / bins, (b + 1) / bins\n        idx = [i for i, c in enumerate(conf) if lo <= c <= hi]\n        if not idx:\n            continue\n        acc = sum(correct[i] for i in idx) / len(idx)\n        avg_conf = sum(conf[i] for i in idx) / len(idx)\n        ece += (len(idx) / n) * abs(acc - avg_conf)\n    return ece\n\ndef compute_auroc(conf: List[float], correct: List[int]) -> Optional[float]:\n    if len(set(correct)) < 2:\n        return None\n    return roc_auc_score(correct, conf)\n", "outputs": []}, {"id": "051ccbe7", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ncfg = ModelCfg(load_in_4bit=True)\nmdl = Llama31Judge(cfg)\n\nq = \"Which gas is most responsible for the greenhouse effect on Earth?\\nA) Oxygen\\nB) Nitrogen\\nC) Carbon dioxide\\nD) Argon\"\nletters = \"ABCD\"\n\n# Step 1: Sample response\nresp = mdl.generate(SHARED_SAMPLING_PROMPT.format(Letters=letters, Question=q))\n\n# Step 2: Extract answer letter\npred_letter = extract_answer_letter(resp)\n\n# Step 3: Strip numeric confidence\ncleaned, nvu = strip_numeric_confidence(resp)\n\n# Step 4: Judge LVU\njudge_prompt = LVU_JUDGE_PROMPT.format(Question=q, Response=cleaned)\njudge_out = mdl.generate(judge_prompt, max_new_tokens=64)\nlvu_conf = parse_lvu_conf(judge_out)\n\nprint(\"RAW RESPONSE:\\n\", resp)\nprint(\"Predicted Answer Letter:\", pred_letter)\nprint(\"NVU (numeric) if present:\", nvu)\nprint(\"LVU Judge Output:\", judge_out)\nprint(\"LVU Confidence [0-1]:\", lvu_conf)\n", "outputs": []}, {"id": "817d5ec3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nnames = [\"Brendan\", \"Amanda\"]\nhedges = [\"almost certain\", \"possible\", \"unlikely\"]\nstatements = [\"they will buy a new watch this Thanksgiving weekend.\", \"their boss owns a blue car.\"]\n\nfor n in names:\n    for h in hedges:\n        for s in statements:\n            sentence = f\"{n} believes it is {h} that {s}\"\n            out = mdl.generate(HEDGING_PROMPT.format(sentence=sentence), max_new_tokens=32)\n            prob = parse_hedge_conf(out)\n            print(sentence, \"->\", out, \"| Parsed:\", prob)\n", "outputs": []}]}